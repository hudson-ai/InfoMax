{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import xavier_initializer\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator and discriminator parameter initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_gen(n_input = 100, n_hidden_1 = 300, n_hidden_2 = 300, n_out = 2,\n",
    "             random_seed = None):\n",
    "    #Initializer\n",
    "    xavier_init = xavier_initializer(seed = random_seed)\n",
    "    \n",
    "    # Network Parameters\n",
    "    gen_params = {}\n",
    "    gen_params['n_input'] = n_input\n",
    "    gen_params['n_hidden_1'] = n_hidden_1 # 1st layer number of features\n",
    "    gen_params['n_hidden_2'] = n_hidden_2 # 2nd layer number of features\n",
    "    gen_params['n_out'] = n_out\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    gen_params['weights'] = {\n",
    "        'w1': tf.Variable(xavier_init([n_input, n_hidden_1]), name = 'g_w1'),\n",
    "        'w2': tf.Variable(xavier_init([n_hidden_1, n_hidden_2]), name = 'g_w2'),\n",
    "        'w_out': tf.Variable(xavier_init([n_hidden_2, n_out]), name = 'g_w_out')\n",
    "    }\n",
    "    gen_params['biases'] = {\n",
    "        'b1': tf.Variable(tf.zeros([n_hidden_1]), name = 'g_b1'),\n",
    "        'b2': tf.Variable(tf.zeros([n_hidden_2]), name = 'g_b1'),\n",
    "        'b_out': tf.Variable(tf.zeros([n_out]), name = 'g_b_out')\n",
    "    }\n",
    "    \n",
    "    #return parameter dict\n",
    "    return(gen_params)\n",
    "\n",
    "def make_disc(n_input = 2, n_hidden_1 = 300, n_hidden_2 = 300,\n",
    "             random_seed = None):\n",
    "    #Initializer\n",
    "    xavier_init = xavier_initializer(seed = random_seed)\n",
    "    \n",
    "    # Network Parameters\n",
    "    disc_params = {}\n",
    "    disc_params['n_input'] = n_input\n",
    "    disc_params['n_hidden_1'] = n_hidden_1 # 1st layer number of features\n",
    "    disc_params['n_hidden_2'] = n_hidden_2 # 2nd layer number of features\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    disc_params['weights'] = {\n",
    "        'w1': tf.Variable(xavier_init([n_input, n_hidden_1]), name = 'd_w1'),\n",
    "        'w2': tf.Variable(xavier_init([n_hidden_1, n_hidden_2]), name = 'd_w2'),\n",
    "        'w_out': tf.Variable(xavier_init([n_hidden_2, 1]), name = 'd_w_out')\n",
    "    }\n",
    "    disc_params['biases'] = {\n",
    "        'b1': tf.Variable(tf.zeros([n_hidden_1]), name = 'd_b1'),\n",
    "        'b2': tf.Variable(tf.zeros([n_hidden_2]), name = 'd_b1'),\n",
    "        'b_out': tf.Variable(tf.zeros([1]), name = 'd_b_out')\n",
    "    }\n",
    "    \n",
    "    #return parameter dict\n",
    "    return(disc_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GAN(object):\n",
    "    def __init__(self, gen_params = None, disc_params = None,\n",
    "                 data_dim = 2, z_dim = 100, \n",
    "                 gen_hidden_1 = 300, gen_hidden_2 = 300,\n",
    "                 disc_hidden_1 = 300, disc_hidden_2 = 300,\n",
    "                 random_seed = None):\n",
    "        \n",
    "        #Generator params, may pass optional initialization\n",
    "        gen_params = gen_params if gen_params is not None else make_gen(\n",
    "            n_input = z_dim, n_hidden_1 = gen_hidden_1, n_hidden_2 = gen_hidden_2, n_out = data_dim, \n",
    "            random_seed = random_seed\n",
    "        )\n",
    "        self.gen_params = gen_params\n",
    "        \n",
    "        z_dim = gen_params['n_input']\n",
    "        self.z_dim = z_dim\n",
    "        g_theta = [theta for theta in gen_params['weights'].values()] + \\\n",
    "                  [theta for theta in gen_params['biases'].values()]\n",
    "        self.g_theta = g_theta\n",
    "            \n",
    "        #Discriminator params, may pass optional initialization\n",
    "        disc_params = disc_params if disc_params is not None else make_disc(\n",
    "            n_input = data_dim, n_hidden_1 = disc_hidden_1, n_hidden_2 = disc_hidden_2,\n",
    "            random_seed = random_seed\n",
    "        )\n",
    "        self.disc_params = disc_params\n",
    "        \n",
    "        x_dim = disc_params['n_input']\n",
    "        d_theta = [theta for theta in disc_params['weights'].values()] + \\\n",
    "                  [theta for theta in disc_params['biases'].values()]\n",
    "        self.d_theta = d_theta\n",
    "            \n",
    "        #GAN graph\n",
    "        X = tf.placeholder(tf.float32, shape=[None, x_dim], name='X')\n",
    "        self.X = X\n",
    "        Z = tf.placeholder(tf.float32, shape=[None, z_dim], name='Z')\n",
    "        self.Z = Z\n",
    "        \n",
    "        G_X = self.gen(Z)\n",
    "        \n",
    "        D_logit_real, D_real = self.disc(X)\n",
    "        self.D_real = D_real\n",
    "        \n",
    "        D_logit_fake, D_fake = self.disc(G_X)\n",
    "        self.D_fake = D_fake\n",
    "\n",
    "    #Generator and discriminator models\n",
    "    def gen(self, z):\n",
    "        #Params\n",
    "        weights = self.gen_params['weights']\n",
    "        biases = self.gen_params['biases']\n",
    "        \n",
    "        #Graph\n",
    "        g_h1 = tf.nn.relu(tf.matmul(z, weights['w1']) + biases['b1'])\n",
    "        g_h2 = tf.nn.relu(tf.matmul(g_h1, weights['w2']) + biases['b2'])\n",
    "        # Output layer with linear activation\n",
    "        g_out = tf.matmul(g_h2, weights['w_out']) + biases['b_out']\n",
    "        \n",
    "        return(g_out)\n",
    "    \n",
    "    def disc(self, x):\n",
    "        #Params\n",
    "        weights = self.disc_params['weights']\n",
    "        biases = self.disc_params['biases']\n",
    "        \n",
    "        #Graph\n",
    "        d_h1 = tf.nn.relu(tf.matmul(x, weights['w1']) + biases['b1'])\n",
    "        d_h2 = tf.nn.relu(tf.matmul(d_h1, weights['w2']) + biases['b2'])\n",
    "        # Logit\n",
    "        d_logit = tf.matmul(d_h2, weights['w_out']) + biases['b_out']\n",
    "        #Probability\n",
    "        d_prob = tf.nn.sigmoid(d_logit)\n",
    "        \n",
    "        return(d_logit, d_prob)\n",
    "    \n",
    "    def sample_Z(self, m):\n",
    "        return np.random.uniform(-1., 1., size=[m, self.z_dim]).astype('float32')\n",
    "    \n",
    "    def sample_G(self, m):\n",
    "        return self.gen(self.sample_Z(m))\n",
    "    \n",
    "    def train(self, x, learning_rate = 0.001, training_epochs = 300, mb_size = 64, display_step = 30):\n",
    "        sess = tf.Session()\n",
    "        self.sess = sess\n",
    "        \n",
    "        N = len(x)\n",
    "        n_batches = N // mb_size\n",
    "        \n",
    "        #Losses\n",
    "        D_loss = -tf.reduce_mean(tf.log(self.D_real) + tf.log(1. - self.D_fake))\n",
    "        G_loss = -tf.reduce_mean(tf.log(self.D_fake))\n",
    "        \n",
    "        #Optimizers\n",
    "        D_solver = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(D_loss, var_list=self.d_theta)\n",
    "        G_solver = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(G_loss, var_list=self.g_theta)\n",
    "        \n",
    "        # Initializing the variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        with sess.as_default():\n",
    "            sess.run(init)\n",
    "            neg_d_losses = []\n",
    "            for epoch in range(training_epochs):\n",
    "                D_loss_avg = 0\n",
    "                if epoch % display_step == 0:\n",
    "                    G_x = self.sample_G(1000).eval()\n",
    "                    plt.figure()\n",
    "                    plt.scatter(x[:,0], x[:,1])\n",
    "                    plt.scatter(G_x[:,0], G_x[:,1])\n",
    "                    plt.show()\n",
    "                    \n",
    "                \n",
    "                for batch in range(n_batches):\n",
    "                    X_mb = x[(batch*mb_size):((batch+1)*mb_size), :]\n",
    "\n",
    "                    _, D_loss_curr = sess.run([D_solver, D_loss], feed_dict={self.X: X_mb, self.Z: self.sample_Z(mb_size)})\n",
    "                    _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict={self.Z: self.sample_Z(mb_size)})\n",
    "                    \n",
    "                    D_loss_avg = D_loss_avg + D_loss_curr/n_batches\n",
    "                neg_d_losses.append(-D_loss_avg)\n",
    "            \n",
    "            plt.figure()\n",
    "            plt.plot(neg_d_losses)\n",
    "            plt.title(\"Negative Discriminator Loss\")\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(\"Negative D_Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "N = 1000\n",
    "X, Y = make_moons(N, noise = .1)\n",
    "X = X.astype('float32')\n",
    "Y = np.array([[1,0] if y else [0,1] for y in Y])\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gan = GAN(\n",
    "    data_dim = 2, z_dim = 2, \n",
    "    gen_hidden_1 = 300, gen_hidden_2 = 300,\n",
    "    disc_hidden_1 = 300, disc_hidden_2 = 300,\n",
    ")\n",
    "gan.train(X, learning_rate = 0.0001, training_epochs = 5000, mb_size = 100, display_step = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
